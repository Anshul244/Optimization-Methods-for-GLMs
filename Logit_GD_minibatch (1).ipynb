{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Logit_GD_minibatch.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6itX3i1_A7WY"},"source":["import os\n","import numpy as np\n","import collections\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import csr_matrix\n","import gensim\n","from scipy import sparse\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import random\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","import string\n","import pandas as pd\n","\n","from pprint import pprint\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","\n","\n","from sklearn.datasets import fetch_20newsgroups\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('wordnet')\n","\n","stemmer = PorterStemmer()\n","\n","def readData(direct):\n","    files = os.listdir(direct)\n","    emails_list =[]\n","    labels_list =[]\n","    emails = [direct + email for email in files]\n","    for email in emails:\n","        email_path = email[:-4]\n","        email_lbl = email_path[-4:]\n","        f = open(email, 'r', encoding=\"utf8\", errors='ignore')\n","        data = f.read()\n","        emails_list.append(data)\n","        if email_lbl == 'spam' :\n","            labels_list.append(1)\n","        elif email_lbl == '.ham':\n","            labels_list.append(0)\n","    return emails_list, labels_list\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result\n","\n","def preprocessing(text,stem=False, stop=False, sent=False):\n","    \n","    \n","    # Remove punctuations\n","    exclude = set(string.punctuation)\n","    text = ''.join(ch for ch in text if ch not in exclude)\n","    \n"," \n","    tokens = word_tokenize(text)\n","    \n","    if stop:\n","        #print(\"Stop\")\n","        stop = stopwords.words('english')\n","        stop.extend(['from', 'subject', 're', 'edu', 'use','the','but','didnt','dont','many','also','us','went','get','know','wasnt','would','one','maxaxaxaxaxaxaxaxaxaxaxaxaxaxax','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v_mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v'])\n","        #print(stop)\n","        tokens = [word.lower() for word in tokens]\n","        \n","        tokens =[word for word in tokens if len(word) > 2 or word.isnumeric() == False]        \n","            \n","        tokens =[word for word in tokens if word not in stop]\n","        \n","        #print(tokens)\n","\n","    if stem:\n","        stemmer = PorterStemmer()\n","        tokens = [stemmer.stem(t) for t in tokens]\n","    \n","    if sent:\n","        tokens = ' '.join(tokens)\n","        \n","    return tokens\n","\n","\n","\n","def clean_news(articles):\n","    \n","    clean = []\n","    dirty = []\n","    count = 0\n","    for article in articles:\n","        \n","        if len(article) > 0:\n","            \n","            clean.append(article)\n","        else :\n","            dirty.append(count)\n","        count += 1\n","        #print clean\n","        \n","        #sys.exit(1)\n","            \n","    return clean, dirty\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDj3yJRlKuXh"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HewyprMyA7Wg"},"source":["datalocation = \"/content/drive/My Drive/emails/\"\n","#datalocation = \"C:/email/Documents/New folder/\"\n","emails_list, labels_list = readData(datalocation)\n","print(len(labels_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhipEvjgh_H0"},"source":["categories1 = ['rec.sport.baseball','sci.space']\n","categories2 = ['sci.med','talk.religion.misc']\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories1, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test',categories=categories1, remove=('headers', 'footers', 'quotes'))\n","\n","newsgroups_train_cat2 = fetch_20newsgroups(subset='train', categories=categories2, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test_cat2 = fetch_20newsgroups(subset='test',categories=categories2, remove=('headers', 'footers', 'quotes'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUFGkM90uVC8"},"source":["type(newsgroups_train.data)\n","\n","newsgroups_cat1_data = newsgroups_cat1.data\n","print(len(newsgroups_cat1_data))\n","newsgroups_test = newsgroups_cat1_data[1190:]\n","newsgroups_train = newsgroups_cat1_data[:1190]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHnjeBspiWpL"},"source":["print(len(newsgroups_train.data))\n","mylist = (newsgroups_train['target_names'])\n","mylist_no = list(set(newsgroups_train['target']))\n","print(mylist)\n","print(mylist_no)\n","\n","\n","print(len(newsgroups_test.data))\n","mylist1 = (newsgroups_test['target_names'])\n","mylist_no1 = list(set(newsgroups_test['target']))\n","print(mylist1)\n","print(mylist_no1)\n","\n","print(len(newsgroups_train_cat2.data))\n","mylist_cat2 = (newsgroups_train_cat2['target_names'])\n","mylist_no_cat2 = list(set(newsgroups_train_cat2['target']))\n","print(mylist_cat2)\n","print(mylist_no_cat2)\n","\n","\n","print(len(newsgroups_test_cat2.data))\n","mylist1_cat2 = (newsgroups_test_cat2['target_names'])\n","mylist_no1_cat2 = list(set(newsgroups_test_cat2['target']))\n","print(mylist1_cat2)\n","print(mylist_no1_cat2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pgVRh2bkCcG"},"source":["targets_train = newsgroups_train.target\n","targets_test = newsgroups_test.target\n","\n","newsgroups_train1, noise_train = clean_news(newsgroups_train.data)\n","newsgroups_test1, noise_test = clean_news(newsgroups_test.data)\n","\n","targets_train_filter = np.delete(targets_train, noise_train)\n","targets_test_new_filter = np.delete(targets_test, noise_test)\n","\n","print(len(targets_train_filter))\n","print(len(targets_test_new_filter))\n","\n","\n","targets_train_cat2 = newsgroups_train_cat2.target\n","targets_test_cat2 = newsgroups_test_cat2.target\n","\n","newsgroups_train1_cat2, noise_train_cat2 = clean_news(newsgroups_train_cat2.data)\n","newsgroups_test1_cat2, noise_test_cat2 = clean_news(newsgroups_test_cat2.data)\n","\n","targets_train_filter_cat2 = np.delete(targets_train_cat2, noise_train_cat2)\n","targets_test_new_filter_cat2 = np.delete(targets_test_cat2, noise_test_cat2)\n","\n","print(len(targets_train_filter_cat2))\n","print(len(targets_test_new_filter_cat2))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xnj6uxrGmhvA"},"source":["dataset_trainA, Labels_trainA = newsgroups_train1, targets_train_filter\n","dataset_testA, Labels_testA = newsgroups_test1, targets_test_new_filter\n","\n","dataset_trainB, Labels_trainB = newsgroups_train1_cat2, targets_train_filter_cat2\n","dataset_testB, Labels_testB = newsgroups_test1_cat2, targets_test_new_filter_cat2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyfYheSNA7Wl"},"source":["#input feature vector\n","vectorizer = TfidfVectorizer(tokenizer=preprocess)\n","email_vect_C = vectorizer.fit_transform(emails_list)\n","print(email_vect_C.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkhPHR0ExmAC"},"source":["#input feature vector\n","vectorizer = TfidfVectorizer(tokenizer=preprocess)\n","#email_vect = vectorizer.fit_transform(emails_list)\n","#print(email_vect.shape)\n","\n","newsgroup_vect_A = vectorizer.fit_transform(dataset_trainA)\n","print(newsgroup_vect_A.shape)\n","print(type(newsgroup_vect_A))\n","\n","# will have to implement\n","\n","newsgroup_vect_A_train = newsgroup_vect_A[:110] \n","print(newsgroup_vect_A_train.shape)\n","\n","newsgroup_vect_test_A = vectorizer.fit_transform(dataset_testA)\n","print(newsgroup_vect_test_A.shape)\n","\n","newsgroup_vect_B = vectorizer.fit_transform(dataset_trainB)\n","print(newsgroup_vect_B.shape)\n","\n","newsgroup_vect_test_B = vectorizer.fit_transform(dataset_testB)\n","print(newsgroup_vect_test_B.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7dtGOGNA7Wp"},"source":["#sigmoid function: Activation function used to map any real value between 0 and 1\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","#Predicts binary labels for a set of emails.\n","def predict(feature_vectors,weights):\n","    pred_sigm_list = []\n","    pred_label_list = []\n","    bias = 1\n","    for email_v in feature_vectors:\n","        weights_T = np.transpose(weights)\n","        \n","        y_pred = np.dot(email_v, weights_T) \n","        #print(y_pred.data)\n","        y_pred = y_pred.data + bias\n","        y_pred_sigm = sigmoid(y_pred)\n","        pred_sigm_list.append(y_pred_sigm)\n","        \n","        if y_pred_sigm >= 0.5:\n","            y_pred_label = 1\n","        else:\n","            y_pred_label = 0\n","        pred_label_list.append(y_pred_label)\n","        \n","        #print(pred_sigm_list[:10])\n","        \n","    return pred_label_list, pred_sigm_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZYtDQXUd-Vt"},"source":["import math\n","\n","def get_exp_decay(k,i,initial_lr):\n","\n","  #print(\"initial_lr: \",initial_lr)\n","  #print(\"k\", k)\n","  #print(\"itteration\", i)\n","\n","  decay = math.exp(-k*i)\n","  curr_lr = initial_lr * decay\n","\n","  #print(\"decay\", decay)\n","  #print(\"curr_lr\", curr_lr)\n","  \n","  return curr_lr, decay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtgDEeoWsGD3"},"source":["#Predicts binary labels for a set of emails.\n","def inference(feature_vectors,weights):\n","    pred_sigm_list = []\n","    pred_label_list = []\n","    bias = 1\n","    for email_v in feature_vectors:\n","        weights_T = np.transpose(weights)\n","        \n","        y_pred = np.dot(email_v, weights_T) \n","        #print(y_pred.data)\n","        y_pred = y_pred.data + bias\n","        y_pred_sigm = sigmoid(y_pred)\n","        pred_sigm_list.append(y_pred_sigm)\n","        \n","        if y_pred_sigm >= 0.5:\n","            y_pred_label = 1\n","        else:\n","            y_pred_label = 0\n","        pred_label_list.append(y_pred_label)\n","        \n","        #print(pred_sigm_list[:10])\n","        \n","    return pred_label_list, pred_sigm_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DI75Msm4A7Wv"},"source":["#loss function  \n","def log_loss(y_pred_sigm, y_true):\n","    #print(y_pred_sigm)\n","    #p = np.clip(predicted, eps, 1 - eps)\n","    #print(\"y_pred_sigm\", y_pred_sigm)\n","    logloss = -(y_true * np.log(y_pred_sigm) + (1 - y_true) * np.log(1 - y_pred_sigm))\n","    return logloss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbvy1TCJA7Wz"},"source":["\n","def update_weights(y_pred_sigm, labels, weights_list, emails, total_emails, total_labels, learning_rate):\n","    for i in range(len(labels)):\n","        weights = weights_list\n","        dw = np.dot(emails[i], y_pred_sigm[i] - labels[i])\n","        dw *= learning_rate\n","\n","        if len(dw) == 0:\n","\n","          #print(i)\n","          #print(y_pred_sigm[i])\n","          dw = np.dot(total_emails[1], 0.01 - total_labels[1])\n","          dw *= learning_rate\n","\n","        weights = weights - dw[0]\n","     \n","     \n","    return weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9gy3Q6k6vnb"},"source":["\n","def get_mini_batch(emails, labels, batch_size, data_size):\n","\n","\n","    if batch_size == 1:\n","      index_no = range(0, data_size)\n","    elif batch_size == 0:\n","      index_no = random.sample(range(0, data_size), 1)\n","    else:\n","      index_no = random.sample(range(0, data_size), batch_size)\n","      \n","    email_list = []\n","    label_list =[]\n","    \n","    for i in index_no:\n","        #print(i)\n","        email_list.append(np.array(emails[i].todense()))\n","        label_list.append(labels[i])\n","\n","    email_batch = np.array(email_list)\n","    email_batch = email_batch[:,0,:]\n","    #print(\"Numpy array shape: \",email_batch.shape)\n","    email_batch_m = csr_matrix(email_batch)\n","    #print(\"CSR Matrix shape: \",email_batch_m.shape)\n","    return email_batch_m, label_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mTmzMPPBkpM"},"source":["def get_mini_batches(emails, labels, batch_size, data_size):\n","    \n","    print(len(labels))\n","    total_batch_no = int(data_size/batch_size)\n","    print(total_batch_no)\n","    \n","    #This list will contain N no of batches each batch having batch_size no of emails..\n","    batch_data_list = []\n","    \n","    #for each batch\n","    for k in range(total_batch_no):\n","        \n","        index_no = random.sample(range(0, data_size), batch_size)\n","        print(index_no)\n","        curr_batch = []\n","        email_list = []\n","        label_list =[]\n","        \n","        \n","        for i in index_no:\n","            email_list.append(np.array(emails[i].todense()))\n","            label_list.append(labels[i])\n","\n","        email_batch = np.array(email_list)\n","        email_batch = email_batch[:,0,:]\n","        #print(\"Numpy array shape: \",email_batch.shape)\n","        email_batch_m = csr_matrix(email_batch)\n","        \n","        curr_batch.append(email_batch_m)\n","        curr_batch.append(label_list)\n","        \n","        batch_data_list.append(curr_batch)\n","    \n","    return batch_data_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yyuln688WRf"},"source":["\n","def train(emails, labels, n_iter, learning_rate, batch_size):\n","    loss_history = []\n","    \n","   \n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","\n","    total_emails = emails\n","    total_labels = labels\n","    \n","    for k in range(n_iter):\n","\n","\n","        email_count = emails.shape\n","        if batch_size > email_count[0]:\n","           print(\"Batch Size should be smaller than the dataset.\")\n","        else:\n","           emails, labels = get_mini_batch(emails, labels, batch_size,email_count[0])\n","\n","\n","        #print(emails.shape)\n","\n","        \n","        #Random Weights for the 1st itter\n","        if k == 0:\n","            main_weights = weights\n","        #Updated Weights for the next itters    \n","        else:\n","            main_weights = updated_weight_values\n","            \n","        \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","\n","\n","        tot_loss = 0\n","        for i in range(len(labels)):\n","            #print(i)\n","            y_pred_sigm = pred_sigm[i]\n","            #print(y_pred_sigm)\n","            if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","            \n","            # Compute the loss over the whole training set.\n","            indv_loss = log_loss(y_pred_sigm, labels[i])\n","            tot_loss += indv_loss\n","            tot_loss /= len(labels)\n","            \n","        loss_history.append(tot_loss)\n","        updated_weight_values = update_weights(pred_sigm, labels, main_weights, emails, total_emails,total_labels, learning_rate)\n","    return loss_history, updated_weight_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4hCLo-OE2wG"},"source":["def train_lr_time_decay(emails, labels, n_iter, learning_rate, batch_size,flag,k):\n","    loss_history = []\n","\n","    lr = learning_rate\n","    decay = lr/n_iter\n","    \n","    print(\"Decay: \", decay)\n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","\n","    total_emails = emails\n","    total_labels = labels\n","    \n","    for k in range(n_iter):\n","\n","\n","        email_count = emails.shape\n","        if batch_size > email_count[0]:\n","           print(\"Batch Size should be smaller than the dataset.\")\n","        else:\n","           emails, labels = get_mini_batch(emails, labels, batch_size,email_count[0])\n","\n","        #Random Weights for the 1st itter\n","        if k == 0:\n","          main_weights = weights\n","       #Updated Weights for the next itters    \n","        else:\n","           #print(\"learing_rate sceduler: \", flag)\n","          if flag == \"exp_decay\":\n","            \n","            lr, decay = get_exp_decay(k,i,learning_rate)\n","          else:\n","            \n","            lr = lr / (1 + decay)\n","\n","          main_weights = updated_weight_values\n","            \n","        \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","\n","\n","        tot_loss = 0\n","        for i in range(len(labels)):\n","            #print(i)\n","            y_pred_sigm = pred_sigm[i]\n","            #print(y_pred_sigm)\n","            if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","            \n","            # Compute the loss over the whole training set.\n","            indv_loss = log_loss(y_pred_sigm, labels[i])\n","            tot_loss += indv_loss\n","            tot_loss /= len(labels)\n","            \n","        loss_history.append(tot_loss)\n","        updated_weight_values = update_weights(pred_sigm, labels, main_weights, emails, total_emails,total_labels, lr)\n","    return loss_history, updated_weight_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19J3YJEGxkej"},"source":["#For datasetC Emails\n","\n","results_ = []\n","n_iter = 50\n","batch_size = 300\n","#i = 0.01\n","loss_history, model = train_lr_time_decay(newsgroup_vect_B, Labels_trainB, n_iter, 0.01, batch_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnHeGrK5K1qI"},"source":["results = []\n","n_iter = 100\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","print(\"lr_range: \", lr_range)\n","\n","\n","min_batch = 40\n","max_batch = 400\n","batch_intrvl = 50\n","batch_size_range = list(np.arange(min_batch, max_batch, batch_intrvl))\n","batch_size_range.append(1)\n","batch_size_range.append(0)\n","\n","print(\"batch_size_range: \", batch_size_range)\n","\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","corpus_sets = [email_vect_C, newsgroup_vect_A, newsgroup_vect_B]\n","corpus_title = ['email_vect_C', 'newsgroup_vect_A', 'newsgroup_vect_B']\n","lables = [labels_list, Labels_trainA, Labels_trainB]\n","\n","\n","for data in range(len(corpus_sets)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for bt in range(len(batch_size_range)):\n","    print(\"For batch size: \", batch_size_range[bt])\n","    print(\" \")\n","    for i in lr_range:\n","\n","      local_results =[]\n","      print (\"For Learning rate: \", i)\n","    print(\" \")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmfv5pMjA7W_"},"source":["results = []\n","n_iter = 500\n","min_lr = 0.901\n","max_lr =1.0\n","lr_step = 0.1\n","\n","min_batch = 40\n","max_batch = 400\n","batch_intrvl = 50\n","#batch_size_range = list(np.arange(min_batch, max_batch, batch_intrvl))\n","\n","batch_size_range = []\n","#batch_size_range.append(1)\n","batch_size_range.append(340)\n","\n","print(\"batch_size_range: \", batch_size_range)\n","\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","lr_scheduler = \"time_decay\"\n","lr_scheduler_exp = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","corpus_sets = [newsgroup_vect_B]\n","corpus_title = ['newsgroup_vect_B']\n","lables = [Labels_trainB]\n","\n","for data in range(len(corpus_sets)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for bt in range(len(batch_size_range)):\n","    print(\"For batch size: \", batch_size_range[bt])\n","    \n","    for i in lr_range:\n","\n","      local_results =[]\n","      print (\"For Learning rate: \", i)\n","      # print (i)\n","      loss_history, model = train(corpus_sets[data], lables[data], n_iter, i, batch_size_range[bt])\n","      loss_history_time_decay, model = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,batch_size_range[bt], lr_scheduler,k_hp)\n","      loss_history_exp_decay, model = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,batch_size_range[bt], lr_scheduler_exp,k_hp)\n","   \n","\n","      \n","      \n","      local_results.append(corpus_title[data])\n","      local_results.append(batch_size_range[bt])\n","      local_results.append(i)\n","      local_results.append(loss_history)\n","      local_results.append(loss_history_time_decay)\n","      local_results.append(loss_history_exp_decay)\n","      print(\"Loss history: \", loss_history)\n","      print(\"loss_history_time_decay: \", loss_history_time_decay)\n","      print(\"loss_history_exp_decay: \", loss_history_exp_decay)\n","      results.append(local_results)\n","\n","      pd.DataFrame(results).to_csv(\"drive/My Drive/Gradient_descent_minibatch_newsgroupB_cont_91.csv\", index = False)\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNxZxv2oA7XD"},"source":["from matplotlib import pyplot\n","for i in range(len(lr_range)):\n","    \n","    #pyplot.plot(range(n_iter), results[i][1], label = str(lr_range[i]) ) \n","\n","    # naming the x axis , label = 'lr_range[i]'\n","    plt.xlabel('Number of Iterations') \n","    # naming the y axis \n","    plt.ylabel('Loss') \n","    # giving a title to my graph \n","    plt.title('Loss vs Iteration Analysis') \n","    plt.plot(results[i][3], label = str(lr_range[i]))\n","pyplot.legend()\n","pyplot.show()"],"execution_count":null,"outputs":[]}]}