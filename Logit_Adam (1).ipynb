{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Logit_Adam.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"1C3WQRsRFv20"},"source":["import os\n","import numpy as np\n","import collections\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import csr_matrix\n","import gensim\n","from scipy import sparse\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import random\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","import string\n","import pandas as pd\n","\n","from pprint import pprint\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","\n","\n","from sklearn.datasets import fetch_20newsgroups\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('wordnet')\n","\n","stemmer = PorterStemmer()\n","\n","def readData(direct):\n","    files = os.listdir(direct)\n","    emails_list =[]\n","    labels_list =[]\n","    emails = [direct + email for email in files]\n","    for email in emails:\n","        email_path = email[:-4]\n","        email_lbl = email_path[-4:]\n","        f = open(email, 'r', encoding=\"utf8\", errors='ignore')\n","        data = f.read()\n","        emails_list.append(data)\n","        if email_lbl == 'spam' :\n","            labels_list.append(1)\n","        elif email_lbl == '.ham':\n","            labels_list.append(0)\n","    return emails_list, labels_list\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result\n","\n","def preprocessing(text,stem=False, stop=False, sent=False):\n","    \n","    \n","    # Remove punctuations\n","    exclude = set(string.punctuation)\n","    text = ''.join(ch for ch in text if ch not in exclude)\n","    \n"," \n","    tokens = word_tokenize(text)\n","    \n","    if stop:\n","        #print(\"Stop\")\n","        stop = stopwords.words('english')\n","        stop.extend(['from', 'subject', 're', 'edu', 'use','the','but','didnt','dont','many','also','us','went','get','know','wasnt','would','one','maxaxaxaxaxaxaxaxaxaxaxaxaxaxax','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v_mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v'])\n","        #print(stop)\n","        tokens = [word.lower() for word in tokens]\n","        \n","        tokens =[word for word in tokens if len(word) > 2 or word.isnumeric() == False]        \n","            \n","        tokens =[word for word in tokens if word not in stop]\n","        \n","        #print(tokens)\n","\n","    if stem:\n","        stemmer = PorterStemmer()\n","        tokens = [stemmer.stem(t) for t in tokens]\n","    \n","    if sent:\n","        tokens = ' '.join(tokens)\n","        \n","    return tokens\n","\n","\n","\n","def clean_news(articles):\n","    \n","    clean = []\n","    dirty = []\n","    count = 0\n","    for article in articles:\n","        \n","        if len(article) > 0 and (article != '\\n'):\n","      \n","          if  article != ' ':\n","\n","            clean.append(article)\n","          else:\n","            print(\"dirty\",count)\n","            dirty.append(count)\n","        else :\n","            dirty.append(count)\n","        count += 1\n","        #print clean\n","        \n","        #sys.exit(1)\n","            \n","    return clean, dirty"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6iA2mroFv2_","executionInfo":{"status":"ok","timestamp":1614160250107,"user_tz":-60,"elapsed":23661,"user":{"displayName":"Pagol Poka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBSFdyPpCm-bJWWTIxb7Smqq9nFp3ZMYd6nm8j=s64","userId":"14670559270605914844"}},"outputId":"e7184906-add5-4150-d8ba-0d202c5eb763"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZK29aaSUFv2_"},"source":["datalocation = \"/content/drive/My Drive/emails/\"\n","emails_list, labels_list = readData(datalocation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0zFw3GkGRyP"},"source":["categories1 = ['rec.sport.baseball','sci.space']\n","categories2 = ['sci.med','talk.religion.misc']\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories1, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test',categories=categories1, remove=('headers', 'footers', 'quotes'))\n","\n","newsgroups_train_cat2 = fetch_20newsgroups(subset='train', categories=categories2, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test_cat2 = fetch_20newsgroups(subset='test',categories=categories2, remove=('headers', 'footers', 'quotes'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_fH0xrO8rE-"},"source":["print(len(newsgroups_train.data))\n","mylist = (newsgroups_train['target_names'])\n","mylist_no = list(set(newsgroups_train['target']))\n","print(mylist)\n","print(mylist_no)\n","\n","\n","print(len(newsgroups_test.data))\n","mylist1 = (newsgroups_test['target_names'])\n","mylist_no1 = list(set(newsgroups_test['target']))\n","print(mylist1)\n","print(mylist_no1)\n","\n","print(len(newsgroups_train_cat2.data))\n","mylist_cat2 = (newsgroups_train_cat2['target_names'])\n","mylist_no_cat2 = list(set(newsgroups_train_cat2['target']))\n","print(mylist_cat2)\n","print(mylist_no_cat2)\n","\n","\n","print(len(newsgroups_test_cat2.data))\n","mylist1_cat2 = (newsgroups_test_cat2['target_names'])\n","mylist_no1_cat2 = list(set(newsgroups_test_cat2['target']))\n","print(mylist1_cat2)\n","print(mylist_no1_cat2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zg8dXnIu8uiM"},"source":["\n","targets_train = newsgroups_train.target\n","targets_test = newsgroups_test.target\n","\n","newsgroups_train1, noise_train = clean_news(newsgroups_train.data)\n","newsgroups_test1, noise_test = clean_news(newsgroups_test.data)\n","\n","targets_train_filter = np.delete(targets_train, noise_train)\n","targets_test_new_filter = np.delete(targets_test, noise_test)\n","\n","print(\"la\", len(targets_train_filter))\n","print(len(targets_test_new_filter))\n","\n","\n","targets_train_cat2 = newsgroups_train_cat2.target\n","targets_test_cat2 = newsgroups_test_cat2.target\n","\n","newsgroups_train1_cat2, noise_train_cat2 = clean_news(newsgroups_train_cat2.data)\n","newsgroups_test1_cat2, noise_test_cat2 = clean_news(newsgroups_test_cat2.data)\n","\n","targets_train_filter_cat2 = np.delete(targets_train_cat2, noise_train_cat2)\n","targets_test_new_filter_cat2 = np.delete(targets_test_cat2, noise_test_cat2)\n","\n","print(\"aa\",len(targets_train_filter_cat2))\n","print(len(targets_test_new_filter_cat2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MC2A_62I8u2B"},"source":["dataset_trainA, Labels_trainA = newsgroups_train1, targets_train_filter\n","dataset_testA, Labels_testA = newsgroups_test1, targets_test_new_filter\n","\n","dataset_trainB, Labels_trainB = newsgroups_train1_cat2, targets_train_filter_cat2\n","dataset_testB, Labels_testB = newsgroups_test1_cat2, targets_test_new_filter_cat2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmKSf9jU8vK7"},"source":["#input feature vector\n","vectorizer = TfidfVectorizer(tokenizer=preprocess)\n","#email_vect = vectorizer.fit_transform(emails_list)\n","#print(email_vect.shape)\n","\n","newsgroup_vect_A = vectorizer.fit_transform(dataset_trainA)\n","print(newsgroup_vect_A.shape)\n","\n","newsgroup_vect_B = vectorizer.fit_transform(dataset_trainB)\n","print(newsgroup_vect_B.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-kQ2aT6Fv3A"},"source":["import math\n","\n","def get_exp_decay(k,i,initial_lr):\n","\n","  #print(\"initial_lr: \",initial_lr)\n","  #print(\"k\", k)\n","  #print(\"itteration\", i)\n","\n","  decay = math.exp(-k*i)\n","  curr_lr = initial_lr * decay\n","\n","  #print(\"decay\", decay)\n","  #print(\"curr_lr\", curr_lr)\n","  \n","  return curr_lr, decay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbeH-MfGFv3A"},"source":["#sigmoid function: Activation function used to map any real value to a value between 0 and 1\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","#Predicts binary labels for a set of emails.\n","def predict(feature_vectors,weights):\n","    pred_sigm_list = []\n","    pred_label_list = []\n","    bias = 1\n","    for email_v in feature_vectors:\n","        weights_T = np.transpose(weights)\n","        \n","        y_pred = np.dot(email_v, weights_T) \n","        #print(y_pred.data)\n","        y_pred = y_pred.data + bias\n","        y_pred_sigm = sigmoid(y_pred)\n","        pred_sigm_list.append(y_pred_sigm)\n","        \n","        if y_pred_sigm >= 0.5:\n","            y_pred_label = 1\n","        else:\n","            y_pred_label = 0\n","        pred_label_list.append(y_pred_label)\n","        \n","       # print(pred_sigm_list[:10])\n","        \n","    return pred_label_list, pred_sigm_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"puVvmz66Fv3B"},"source":["#loss function  \n","def log_loss(y_pred_sigm, y_true):\n","    #print(y_pred_sigm)\n","    #p = np.clip(predicted, eps, 1 - eps)\n","    logloss = -(y_true * np.log(y_pred_sigm) + (1 - y_true) * np.log(1 - y_pred_sigm))\n","    return logloss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkZhecTlFv3C"},"source":["#Update funtion for Gradient Descent with Momentum\n","def update_momentum(y_pred_sigm, labels, weights_list, emails, learning_rate, update, mom_factor):\n","     for i in range(len(labels)):\n","        weights = weights_list\n","        \n","        \n","        dw = np.dot(emails[i], y_pred_sigm[i] - labels[i])\n","        dw /= len(labels)\n","        dw *= learning_rate\n","        \n","        mom_update = (mom_factor * update) + dw[0]\n","        weights = weights - mom_update\n","        weights = sparse.csr_matrix(weights)\n","        \n","    \n","     return weights, mom_update"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dot-oQ7bFv3C"},"source":["#Update funtion for Gradient Descent with Momentum\n","def update_momentum_adam(y_pred_sigm, labels, weights_list, emails, learning_rate, update_mt, update_vt, beta1, beta2, epsilon):\n","    \n","    \n","    #print(update_mt)\n","    #print(update_vt)\n","    for i in range(len(labels)):\n","        weights = weights_list\n","        \n","        \n","        dw = np.dot(emails[i], y_pred_sigm[i] - labels[i])\n","        dw /= len(labels)\n","        dw *= learning_rate\n","\n","        if len(dw) == 0:\n","          dw = np.dot(emails[i-1], y_pred_sigm[i-1] - labels[i-1])\n","          dw *= learning_rate\n","        \n","                        \n","        curr_mt = beta1 * update_mt + (1 - beta1) * dw[0]\n","        #We have to change this..... dw[0] square karna hai....\n","        curr_vt = beta2 * update_vt + (1 - beta2) * (dw[0].power(2))\n","        \n","        \n","        curr_vt = np.squeeze(np.asarray(curr_vt))\n","        curr_mt = np.squeeze(np.asarray(curr_mt))\n","        \n","        \n","        #We have to check why invalid values are coming in np.sqrt\n","        update_main =  (learning_rate/(np.sqrt(curr_vt) + epsilon)) * curr_mt\n","        #update_main = (learning_rate/(np.sqrt(update_vt) + epsilon)) * update_mt\n","        weights = weights - update_main\n","        weights = sparse.csr_matrix(weights)\n","        \n","        \n","    return weights, curr_vt, curr_mt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sO5Zot6TVF0A"},"source":["def train_lr_time_decay(emails, labels, n_iter, learning_rate, beta1, beta2,flag,k):\n","    \n","    #beta1 = 0.9\n","    #beta2 = 0.999\n","    epsilon = 1e-08\n","    \n","    \n","\n","    lr = learning_rate\n","    decay = lr/n_iter\n","    \n","    loss_history = []\n","    mom_factor = 0.9\n","    \n","\n","    print(\"Lr Decay: \", decay)\n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","    update_vt = np.zeros(emails[0].shape)\n","    update_mt = np.zeros(emails[0].shape)\n","    #print(type(update_vt))\n","   \n","    for k in range(n_iter):\n","       # print(\"Itteration: \", k)\n","        \n","        #Random Weights for the 1st itter\n","        if k == 0:\n","            main_weights = weights\n","            main_updates_vt = update_vt\n","            main_updates_mt = update_mt\n","            \n","        #Updated Weights for the next itters    \n","        else:\n","\n","       #   print(\"learing_rate sceduler: \", flag)\n","          if flag == \"exp_decay\":\n","            \n","            lr, decay = get_exp_decay(k,i,learning_rate)\n","          else:\n","            \n","            lr = lr / (1 + decay)\n","            \n","          main_weights = updated_weight_values\n","          main_updates_vt = update_vt\n","          main_updates_mt = update_mt\n","            \n","            \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","        tot_loss = 0\n","\n","        for i in range(len(labels)):\n","            y_pred_sigm = pred_sigm[i]\n","\n","            if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","            \n","            # Compute the loss over the whole training set.\n","            indv_loss = log_loss(y_pred_sigm, labels[i])\n","            tot_loss += indv_loss\n","            tot_loss /= len(labels)\n","        #print(tot_loss)\n","        loss_history.append(tot_loss)\n","        \n","        if k == 0:\n","            \n","            update =  (lr/(np.sqrt(main_updates_vt) + epsilon)) * main_updates_mt\n","           \n","            #print(kaka[0])\n","            #updated_weight_values =  main_weights - 0\n","            updated_weight_values =  main_weights - update[0]\n","            updated_weight_values = sparse.csr_matrix(updated_weight_values)\n","            #print(updated_weight_values)\n","            \n","        else:\n","            \n","            #updated_weight_values, updated_t = update_momentum(pred_sigm, labels, main_weights, emails, learning_rate, main_updates_vt, mom_factor)\n","            updated_weight_values, update_vt, update_mt = update_momentum_adam(pred_sigm, labels, main_weights, emails, lr, main_updates_mt, main_updates_vt, beta1, beta2, epsilon)\n","            #print(update_mt)\n","            #print(update_vt)\n","            #print(updated_weight_values)\n","            \n","            \n","    return loss_history\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fM4DV0xhFv3D"},"source":["def train(emails, labels, n_iter, learning_rate, beta1, beta2):\n","    \n","    #beta1 = 0.9\n","    #beta2 = 0.999\n","    epsilon = 1e-08\n","    \n","    \n","    \n","    loss_history = []\n","    mom_factor = 0.9\n","    \n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","    update_vt = np.zeros(emails[0].shape)\n","    update_mt = np.zeros(emails[0].shape)\n","    #print(type(update_vt))\n","   \n","    for k in range(n_iter):\n","        #print(\"Itteration: \", k)\n","        \n","        #Random Weights for the 1st itter\n","        if k == 0:\n","            main_weights = weights\n","            main_updates_vt = update_vt\n","            main_updates_mt = update_mt\n","            \n","        #Updated Weights for the next itters    \n","        else:\n","            main_weights = updated_weight_values\n","            main_updates_vt = update_vt\n","            main_updates_mt = update_mt\n","            \n","            \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","        tot_loss = 0\n","\n","        for i in range(len(labels)):\n","            y_pred_sigm = pred_sigm[i]\n","\n","            if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","            \n","            # Compute the loss over the whole training set.\n","            indv_loss = log_loss(y_pred_sigm, labels[i])\n","            tot_loss += indv_loss\n","            tot_loss /= len(labels)\n","        #print(tot_loss)\n","        loss_history.append(tot_loss)\n","        \n","        if k == 0:\n","            \n","            update =  (learning_rate/(np.sqrt(main_updates_vt) + epsilon)) * main_updates_mt\n","           \n","            #print(kaka[0])\n","            #updated_weight_values =  main_weights - 0\n","            updated_weight_values =  main_weights - update[0]\n","            updated_weight_values = sparse.csr_matrix(updated_weight_values)\n","            #print(updated_weight_values)\n","            \n","        else:\n","            \n","            #updated_weight_values, updated_t = update_momentum(pred_sigm, labels, main_weights, emails, learning_rate, main_updates_vt, mom_factor)\n","            updated_weight_values, update_vt, update_mt = update_momentum_adam(pred_sigm, labels, main_weights, emails, learning_rate, main_updates_mt, main_updates_vt, beta1, beta2, epsilon)\n","            #print(update_mt)\n","            #print(update_vt)\n","            #print(updated_weight_values)\n","            \n","            \n","    return loss_history\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09rbuSaw-0gP"},"source":["#step based decay\n","results = []\n","n_iter = 10\n","\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","min_beta1 = 0.001\n","max_beta1 =1.0\n","beta1_step = 0.1\n","beta1_range = list(np.arange(min_beta1, max_beta1, beta1_step))\n","\n","min_beta2 = 0.001\n","max_beta2 =1.0\n","beta2_step = 0.1\n","beta2_range = list(np.arange(min_beta2, max_beta2, beta2_step))\n","beta2_range.append(0.999)\n","\n","\n","corpus_sets = [newsgroup_vect_A, newsgroup_vect_B]\n","corpus_title = ['newsgroup_vect_A', 'newsgroup_vect_B']\n","lables = [Labels_trainA, Labels_trainB]\n","\n","\n","print(\"lr_range: \", lr_range)\n","print(\"beta1_range: \", beta1_range)\n","print(\"beta2_range: \", beta2_range)\n","\n","\n","for data in range(len(corpus_sets)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for b1 in range(len(beta1_range)):\n","    print(\"For Beta1: \", beta1_range[b1])\n","    print(\" \")\n","\n","    for b2 in range(len(beta2_range)):\n","      print(\"For Beta2: \", beta2_range[b2])\n","      print(\" \")\n","\n","      for i in lr_range:\n","        print (\"For Learning rate: \", i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMmg70tc-015"},"source":["#step based decay\n","results = []\n","n_iter = 100\n","\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","\n","lr_scheduler = \"time_decay\"\n","lr_scheduler_exp = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","min_beta1 = 0.301\n","max_beta1 =1.0\n","beta1_step = 0.1\n","beta1_range = list(np.arange(min_beta1, max_beta1, beta1_step))\n","\n","min_beta2 = 0.701\n","max_beta2 =1.0\n","beta2_step = 0.1\n","beta2_range = list(np.arange(min_beta2, max_beta2, beta2_step))\n","beta2_range.append(0.999)\n","\n","\n","corpus_sets = [newsgroup_vect_A]\n","corpus_title = ['newsgroup_vect_A']\n","lables = [Labels_trainA]\n","\n","\n","print(\"lr_range: \", lr_range)\n","print(\"beta1_range: \", beta1_range)\n","print(\"beta2_range: \", beta2_range)\n","\n","\n","for data in range(len(corpus_sets)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for b1 in range(len(beta1_range)):\n","    print(\"For Beta1: \", beta1_range[b1])\n","    print(\" \")\n","\n","    for b2 in range(len(beta2_range)):\n","      print(\"For Beta2: \", beta2_range[b2])\n","      print(\" \")\n","\n","      for i in lr_range:\n","        print (\"For Learning rate: \", i)\n","        local_results =[]\n","\n","        \n","        #loss_history = train(corpus_sets[data], lables[data], n_iter, i,beta1_range[b1], beta2_range[b2])\n","        #loss_history_time_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,beta1_range[b1], beta2_range[b2],lr_scheduler,k_hp)\n","        #loss_history_exp_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,beta1_range[b1], beta2_range[b2],lr_scheduler_exp,k_hp)\n","\n","        loss_history = np.arange(1,1001).tolist()\n","        loss_history_time_decay = np.arange(1001,2001).tolist()\n","        loss_history_exp_decay = np.arange(2001,3001).tolist()\n","\n","        local_results.append(corpus_title[data])\n","        local_results.append(beta1_range[b1])\n","        local_results.append(beta2_range[b2])\n","        local_results.append(i)\n","        local_results.append(loss_history)\n","        local_results.append(loss_history_time_decay)\n","        local_results.append(loss_history_exp_decay)\n","        print(\"Loss history: \", loss_history)\n","        print(\"loss_history_time_decay: \", loss_history_time_decay)\n","        print(\"loss_history_exp_decay: \", loss_history_exp_decay)\n","        results.append(local_results)\n","        pd.DataFrame(results).to_csv(\"drive/My Drive/Results/Logit_ADAM_newsgroupA.csv\", index = False)\n","\n","\n","\n","\n","      print(\" \")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PveSlTmHPG6N"},"source":[""]},{"cell_type":"code","metadata":{"id":"5gpg5b5ZFv3F"},"source":["for i in range(len(loss_history)): \n","    if i % 10 == 0:\n","        print (\"Iteration:\", i, \"Loss:\", loss_history[i]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qzw503cwFv3G"},"source":["plt.plot(range(n_iter), loss_history)  \n","\n","# naming the x axis \n","plt.xlabel('Number of Iterations') \n","# naming the y axis \n","plt.ylabel('Loss') \n","# giving a title to my graph \n","plt.title('Loss vs Iteration Analysis') \n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5isXvrFFv3H"},"source":["#step based decay\n","results = []\n","n_iter = 100\n","\n","lr_range = []\n","lr_range.append(.01)\n","lr_range.append(.101)\n","\n","\n","lr_scheduler = \"time_decay\"\n","lr_scheduler_exp = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","min_beta1 = 0.301\n","max_beta1 =1.0\n","beta1_step = 0.1\n","beta1_range = [0.9001]\n","\n","\n","min_beta2 = 0.701\n","max_beta2 =1.0\n","beta2_step = 0.1\n","beta2_range = [0.99901]\n","\n","\n","\n","#corpus_sets = [newsgroup_vect_B,newsgroup_vect_B,newsgroup_vect_B]\n","corpus_title = ['email_vect_C','newsgroup_vect_A','newsgroup_vect_B']\n","#lables = [Labels_trainB]\n","\n","\n","print(\"lr_range: \", lr_range)\n","print(\"beta1_range: \", beta1_range)\n","print(\"beta2_range: \", beta2_range)\n","\n","\n","for data in range(len(corpus_title)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for b1 in range(len(beta1_range)):\n","    print(\"For Beta1: \", beta1_range[b1])\n","    print(\" \")\n","\n","    for b2 in range(len(beta2_range)):\n","      print(\"For Beta2: \", beta2_range[b2])\n","      print(\" \")\n","\n","      for i in lr_range:\n","        print (\"For Learning rate: \", i)\n","        local_results =[]\n","\n","        \n","        #loss_history = train(corpus_sets[data], lables[data], n_iter, i,beta1_range[b1], beta2_range[b2])\n","        #loss_history_time_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,beta1_range[b1], beta2_range[b2],lr_scheduler,k_hp)\n","        #loss_history_exp_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,beta1_range[b1], beta2_range[b2],lr_scheduler_exp,k_hp)\n","\n","        loss_history = np.arange(1,10).tolist()\n","        loss_history_time_decay = np.arange(10,20).tolist()\n","        loss_history_exp_decay = np.arange(20,30).tolist()\n","\n","        local_results.append(corpus_title[data])\n","        local_results.append(beta1_range[b1])\n","        local_results.append(beta2_range[b2])\n","        local_results.append(i)\n","        local_results.append(loss_history)\n","        local_results.append(loss_history_time_decay)\n","        local_results.append(loss_history_exp_decay)\n","        print(\"Loss history: \", loss_history)\n","        print(\"loss_history_time_decay: \", loss_history_time_decay)\n","        print(\"loss_history_exp_decay: \", loss_history_exp_decay)\n","        results.append(local_results)\n","        pd.DataFrame(results).to_csv(\"drive/My Drive/Results/Logit_ADAM_Default.csv\", index = False)\n","\n","    "],"execution_count":null,"outputs":[]}]}