{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Logit_GD_with_Momentum.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"f4zaXIefIli7"},"source":["import os\n","import numpy as np\n","import collections\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import csr_matrix\n","import gensim\n","from scipy import sparse\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import random\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","import string\n","\n","from pprint import pprint\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","\n","\n","from sklearn.datasets import fetch_20newsgroups\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('wordnet')\n","\n","stemmer = PorterStemmer()\n","\n","def readData(direct):\n","    files = os.listdir(direct)\n","    emails_list =[]\n","    labels_list =[]\n","    emails = [direct + email for email in files]\n","    for email in emails:\n","        email_path = email[:-4]\n","        email_lbl = email_path[-4:]\n","        f = open(email, 'r', encoding=\"utf8\", errors='ignore')\n","        data = f.read()\n","        emails_list.append(data)\n","        if email_lbl == 'spam' :\n","            labels_list.append(1)\n","        elif email_lbl == '.ham':\n","            labels_list.append(0)\n","    return emails_list, labels_list\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result\n","\n","def preprocessing(text,stem=False, stop=False, sent=False):\n","    \n","    \n","    # Remove punctuations\n","    exclude = set(string.punctuation)\n","    text = ''.join(ch for ch in text if ch not in exclude)\n","    \n"," \n","    tokens = word_tokenize(text)\n","    \n","    if stop:\n","        #print(\"Stop\")\n","        stop = stopwords.words('english')\n","        stop.extend(['from', 'subject', 're', 'edu', 'use','the','but','didnt','dont','many','also','us','went','get','know','wasnt','would','one','maxaxaxaxaxaxaxaxaxaxaxaxaxaxax','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v_mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v'])\n","        #print(stop)\n","        tokens = [word.lower() for word in tokens]\n","        \n","        tokens =[word for word in tokens if len(word) > 2 or word.isnumeric() == False]        \n","            \n","        tokens =[word for word in tokens if word not in stop]\n","        \n","        #print(tokens)\n","\n","    if stem:\n","        stemmer = PorterStemmer()\n","        tokens = [stemmer.stem(t) for t in tokens]\n","    \n","    if sent:\n","        tokens = ' '.join(tokens)\n","        \n","    return tokens\n","\n","\n","\n","def clean_news(articles):\n","    \n","    clean = []\n","    dirty = []\n","    count = 0\n","    for article in articles:\n","        \n","        if len(article) > 0 and (article != '\\n'):\n","      \n","          if  article != ' ':\n","\n","            clean.append(article)\n","          else:\n","            print(\"dirty\",count)\n","            dirty.append(count)\n","        else :\n","            dirty.append(count)\n","        count += 1\n","        #print clean\n","        \n","        #sys.exit(1)\n","            \n","    return clean, dirty"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4AEsxi8JNtE"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrW1gKZPIljC"},"source":["categories1 = ['rec.sport.baseball','sci.space']\n","categories2 = ['sci.med','talk.religion.misc']\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories1, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test',categories=categories1, remove=('headers', 'footers', 'quotes'))\n","\n","newsgroups_train_cat2 = fetch_20newsgroups(subset='train', categories=categories2, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test_cat2 = fetch_20newsgroups(subset='test',categories=categories2, remove=('headers', 'footers', 'quotes'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqUrWJ_8KSUB"},"source":["print(len(newsgroups_train.data))\n","mylist = (newsgroups_train['target_names'])\n","mylist_no = list(set(newsgroups_train['target']))\n","print(mylist)\n","print(mylist_no)\n","\n","\n","print(len(newsgroups_test.data))\n","mylist1 = (newsgroups_test['target_names'])\n","mylist_no1 = list(set(newsgroups_test['target']))\n","print(mylist1)\n","print(mylist_no1)\n","\n","print(len(newsgroups_train_cat2.data))\n","mylist_cat2 = (newsgroups_train_cat2['target_names'])\n","mylist_no_cat2 = list(set(newsgroups_train_cat2['target']))\n","print(mylist_cat2)\n","print(mylist_no_cat2)\n","\n","\n","print(len(newsgroups_test_cat2.data))\n","mylist1_cat2 = (newsgroups_test_cat2['target_names'])\n","mylist_no1_cat2 = list(set(newsgroups_test_cat2['target']))\n","print(mylist1_cat2)\n","print(mylist_no1_cat2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZyaGokPKcXk"},"source":["targets_train = newsgroups_train.target\n","targets_test = newsgroups_test.target\n","\n","newsgroups_train1, noise_train = clean_news(newsgroups_train.data)\n","newsgroups_test1, noise_test = clean_news(newsgroups_test.data)\n","\n","targets_train_filter = np.delete(targets_train, noise_train)\n","targets_test_new_filter = np.delete(targets_test, noise_test)\n","\n","print(\"la\", len(targets_train_filter))\n","print(len(targets_test_new_filter))\n","\n","\n","targets_train_cat2 = newsgroups_train_cat2.target\n","targets_test_cat2 = newsgroups_test_cat2.target\n","\n","newsgroups_train1_cat2, noise_train_cat2 = clean_news(newsgroups_train_cat2.data)\n","newsgroups_test1_cat2, noise_test_cat2 = clean_news(newsgroups_test_cat2.data)\n","\n","targets_train_filter_cat2 = np.delete(targets_train_cat2, noise_train_cat2)\n","targets_test_new_filter_cat2 = np.delete(targets_test_cat2, noise_test_cat2)\n","\n","print(\"aa\",len(targets_train_filter_cat2))\n","print(len(targets_test_new_filter_cat2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDReFfsHK8yq"},"source":["dataset_trainA, Labels_trainA = newsgroups_train1, targets_train_filter\n","dataset_testA, Labels_testA = newsgroups_test1, targets_test_new_filter\n","\n","\n","dataset_trainB, Labels_trainB = newsgroups_train1_cat2, targets_train_filter_cat2\n","dataset_testB, Labels_testB = newsgroups_test1_cat2, targets_test_new_filter_cat2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3bsgJeGIljD"},"source":["#input feature vector\n","vectorizer = TfidfVectorizer(tokenizer=preprocess)\n","#email_vect = vectorizer.fit_transform(emails_list)\n","#print(email_vect.shape)\n","\n","newsgroup_vect_A = vectorizer.fit_transform(dataset_trainA)\n","print(newsgroup_vect_A.shape)\n","\n","newsgroup_vect_B = vectorizer.fit_transform(dataset_trainB)\n","print(newsgroup_vect_B.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqkE_jFZIljD"},"source":["#sigmoid function: Activation function used to map any real value to a value between 0 and 1\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","#Predicts binary labels for a set of emails.\n","def predict(feature_vectors,weights):\n","    pred_sigm_list = []\n","    pred_label_list = []\n","    bias = 1\n","    for email_v in feature_vectors:\n","        weights_T = np.transpose(weights)\n","        \n","        y_pred = np.dot(email_v, weights_T) \n","        #print(y_pred.data)\n","        y_pred = y_pred.data + bias\n","        y_pred_sigm = sigmoid(y_pred)\n","        pred_sigm_list.append(y_pred_sigm)\n","        \n","        if y_pred_sigm >= 0.5:\n","            y_pred_label = 1\n","        else:\n","            y_pred_label = 0\n","        pred_label_list.append(y_pred_label)\n","        \n","       # print(pred_sigm_list[:10])\n","        \n","    return pred_label_list, pred_sigm_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWsNAcf4iX0f"},"source":["import math\n","\n","def get_exp_decay(k,i,initial_lr):\n","\n","  #print(\"initial_lr: \",initial_lr)\n","  #print(\"k\", k)\n","  #print(\"itteration\", i)\n","\n","  decay = math.exp(-k*i)\n","  curr_lr = initial_lr * decay\n","\n","  #print(\"decay\", decay)\n","  #print(\"curr_lr\", curr_lr)\n","  \n","  return curr_lr, decay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKhYU6LYIljD"},"source":["#loss function  \n","def log_loss(y_pred_sigm, y_true):\n","    #print(y_pred_sigm)\n","    #p = np.clip(predicted, eps, 1 - eps)\n","    logloss = -(y_true * np.log(y_pred_sigm) + (1 - y_true) * np.log(1 - y_pred_sigm))\n","    return logloss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUQV_PWBIljD"},"source":["#Update funtion for Gradient Descent with Momentum\n","def update_momentum(y_pred_sigm, labels, weights_list, emails, learning_rate, update, mom_factor):\n","     for i in range(len(labels)):\n","        weights = weights_list\n","        dw = np.dot(emails[i], y_pred_sigm[i] - labels[i])\n","        dw /= len(labels)\n","        dw *= learning_rate\n","\n","        if len(dw) == 0:\n","          dw = np.dot(emails[i-1], y_pred_sigm[i-1] - labels[i-1])\n","          dw *= learning_rate\n","        \n","      \n","        mom_update = (mom_factor * update) + dw[0]\n","        weights = weights - mom_update\n","        weights = sparse.csr_matrix(weights)\n","        \n","    \n","     return weights, mom_update"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vy6VxdXtIljE"},"source":["def train_lr_time_decay(emails, labels, n_iter, learning_rate,mom_factor,flag,k):\n","    loss_history = []\n","    \n","    \n","\n","    lr = learning_rate\n","    decay = lr/n_iter\n","    print(\"Lr Decay: \", decay)\n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","    update = np.zeros(emails[0].shape)\n","   \n","    for i in range(n_iter):\n","        #print(\"Itteration: \", i)\n","        \n","        #Random Weights for the 1st itter\n","        if i == 0:\n","          main_weights = weights\n","          main_updates = update \n","        #Updated Weights for the next itters    \n","        else:\n","          print(\"learing_rate sceduler: \", flag)\n","          if flag == \"exp_decay\":\n","            \n","            lr, decay = get_exp_decay(k,i,learning_rate)\n","          else:\n","            \n","            lr = lr / (1 + decay)\n","          main_weights = updated_weight_values\n","          main_updates = updated_t\n","        \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","        tot_loss = 0\n","        for i in range(len(labels)):\n","            y_pred_sigm = pred_sigm[i]\n","\n","            if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","            \n","            # Compute the loss over the whole training set.\n","            indv_loss = log_loss(y_pred_sigm, labels[i])\n","            tot_loss += indv_loss\n","            tot_loss /= len(labels)\n","        loss_history.append(tot_loss)\n","        if i == 0:\n","            updated_weight_values =  main_weights - main_updates\n","        else:\n","            updated_weight_values, updated_t = update_momentum(pred_sigm, labels, main_weights, emails, lr, main_updates, mom_factor)\n","            \n","            \n","    return loss_history\n","   \n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmdzzMRM9XRP"},"source":["def train(emails, labels, n_iter, learning_rate,mom_factor):\n","    loss_history = []\n","    \n","    \n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","    update = np.zeros(emails[0].shape)\n","   \n","    for i in range(n_iter):\n","        #print(\"Itteration: \", i)\n","        \n","        #Random Weights for the 1st itter\n","        if i == 0:\n","            main_weights = weights\n","            main_updates = update \n","        #Updated Weights for the next itters    \n","        else:\n","            main_weights = updated_weight_values\n","            main_updates = updated_t\n","        \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","        tot_loss = 0\n","        for i in range(len(labels)):\n","            y_pred_sigm = pred_sigm[i]\n","\n","            if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","            \n","            # Compute the loss over the whole training set.\n","            indv_loss = log_loss(y_pred_sigm, labels[i])\n","            tot_loss += indv_loss\n","            tot_loss /= len(labels)\n","        loss_history.append(tot_loss)\n","        if i == 0:\n","            updated_weight_values =  main_weights - main_updates\n","        else:\n","            updated_weight_values, updated_t = update_momentum(pred_sigm, labels, main_weights, emails, learning_rate, main_updates, mom_factor)\n","            \n","            \n","    return loss_history\n","   \n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QC0dyAabzCp"},"source":["#step based decay\n","results = []\n","n_iter = 10\n","\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","min_momentum = 0.001\n","max_momentum =1.0\n","momentum_step = 0.1\n","momentum_range = list(np.arange(min_momentum, max_momentum, momentum_step))\n","\n","\n","corpus_sets = [newsgroup_vect_A, newsgroup_vect_B]\n","corpus_title = ['newsgroup_vect_A', 'newsgroup_vect_B']\n","lables = [Labels_trainA, Labels_trainB]\n","\n","\n","for data in range(len(corpus_sets)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for mf in range(len(momentum_range)):\n","    print(\"For Momentum Factor: \", momentum_range[mf])\n","    print(\" \")\n","    for i in lr_range:\n","\n","      local_results =[]\n","      print (\"For Learning rate: \", i)\n","\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdtF3J-uQdl7"},"source":["#step based decay\n","results = []\n","n_iter = 10\n","\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","lr_scheduler = \"time_decay\"\n","# lr_scheduler = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","min_momentum = 0.001\n","max_momentum =1.0\n","momentum_step = 0.1\n","momentum_range = list(np.arange(min_momentum, max_momentum, momentum_step))\n","\n","#corpus_sets = [email_vect_C,newsgroup_vect_A, newsgroup_vect_B]\n","#corpus_title = ['email_vect_C','newsgroup_vect_A', 'newsgroup_vect_B']\n","#lables = [labels_list, Labels_trainA, Labels_trainB]\n","\n","corpus_sets = [newsgroup_vect_A, newsgroup_vect_B]\n","corpus_title = ['newsgroup_vect_A', 'newsgroup_vect_B']\n","lables = [Labels_trainA, Labels_trainB]\n","\n","\n","for data in range(len(corpus_sets)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for mf in range(len(momentum_range)):\n","    print(\"For Momentum Factor: \", momentum_range[mf])\n","\n","    for i in lr_range:\n","\n","      local_results =[]\n","      print (\"For Learning rate: \", i)\n","\n","      #loss_history = train(corpus_sets[data], lables[data], n_iter, i,momentum_range[mf])\n","      #loss_history = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,momentum_range[mf], lr_scheduler,k_hp)\n","\n","      loss_history = np.arange(1,1001).tolist()\n","      loss_history_exp = np.arange(1001,2001).tolist()\n","      loss_history_time = np.arange(2001,3001).tolist()\n","\n","      local_results.append(corpus_title[data])\n","      local_results.append(momentum_range[mf])\n","      local_results.append(i)\n","      local_results.append(loss_history)\n","      local_results.append(loss_history_exp)\n","      local_results.append(loss_history_time)\n","      print(\"Loss history: \", loss_history)\n","      results.append(local_results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEfa8WG2wD7s"},"source":["#step based decay\n","results = []\n","n_iter = 10\n","\n","lr_range = []\n","lr_range.append(.01)\n","lr_range.append(.101)\n","\n","lr_scheduler = \"time_decay\"\n","# lr_scheduler = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","min_momentum = 0.001\n","max_momentum =1.0\n","momentum_step = 0.1\n","momentum_range = [.9001]\n","\n","#corpus_sets = [email_vect_C,newsgroup_vect_A, newsgroup_vect_B]\n","#corpus_title = ['email_vect_C','newsgroup_vect_A', 'newsgroup_vect_B']\n","#lables = [labels_list, Labels_trainA, Labels_trainB]\n","\n","#corpus_sets = [newsgroup_vect_B,newsgroup_vect_B,newsgroup_vect_B]\n","corpus_title = ['email_vect_C','newsgroup_vect_A','newsgroup_vect_B']\n","#lables = [Labels_trainB]\n","\n","\n","for data in range(len(corpus_title)): \n","  print(\"For Dataset: \", corpus_title[data])\n","\n","  for mf in range(len(momentum_range)):\n","    print(\"For Momentum Factor: \", momentum_range[mf])\n","\n","    for i in lr_range:\n","\n","      local_results =[]\n","      print (\"For Learning rate: \", i)\n","\n","      #loss_history = train(corpus_sets[data], lables[data], n_iter, i,momentum_range[mf])\n","      #loss_history = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i,momentum_range[mf], lr_scheduler,k_hp)\n","\n","      loss_history = np.arange(1,1001).tolist()\n","      loss_history_exp = np.arange(1001,2001).tolist()\n","      loss_history_time = np.arange(2001,3001).tolist()\n","\n","      local_results.append(corpus_title[data])\n","      local_results.append(momentum_range[mf])\n","      local_results.append(i)\n","      local_results.append(loss_history)\n","      local_results.append(loss_history_exp)\n","      local_results.append(loss_history_time)\n","      print(\"Loss history: \", loss_history)\n","      results.append(local_results)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQmjRg-hIljE"},"source":["import pandas as pd\n","pd.DataFrame(results).to_csv(\"drive/My Drive/Results/Gradient_descent_momentum_default.csv\", index = False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkmBCgTjIljE"},"source":["for i in range(len(loss_history)): \n","    if i % 10 == 0:\n","        print (\"Iteration:\", i, \"Loss:\", loss_history[i]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVVRRMWhIljF"},"source":["from matplotlib import pyplot\n","for i in range(len(lr_range)):\n","    \n","    #pyplot.plot(range(n_iter), results[i][1], label = str(lr_range[i]) ) \n","\n","    # naming the x axis , label = 'lr_range[i]'\n","    plt.xlabel('Number of Iterations') \n","    # naming the y axis \n","    plt.ylabel('Loss') \n","    # giving a title to my graph \n","    plt.title('Loss vs Iteration Analysis') \n","    plt.plot(results[i][3], label = str(lr_range[i]))\n","pyplot.legend()\n","pyplot.show()"],"execution_count":null,"outputs":[]}]}