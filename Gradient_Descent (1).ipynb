{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Gradient_Descent.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"mQF0ZvZFP7rY"},"source":["import os\n","import numpy as np\n","import collections\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import csr_matrix\n","import gensim\n","from scipy import sparse\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import random\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","import string\n","import pandas as pd\n","\n","from pprint import pprint\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","\n","import math\n","\n","\n","from sklearn.datasets import fetch_20newsgroups\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('wordnet')\n","\n","stemmer = PorterStemmer()\n","\n","def readData(direct):\n","    files = os.listdir(direct)\n","    emails_list =[]\n","    labels_list =[]\n","    emails = [direct + email for email in files]\n","    for email in emails:\n","        email_path = email[:-4]\n","        email_lbl = email_path[-4:]\n","        f = open(email, 'r', encoding=\"utf8\", errors='ignore')\n","        data = f.read()\n","        emails_list.append(data)\n","        if email_lbl == 'spam' :\n","            labels_list.append(1)\n","        elif email_lbl == '.ham':\n","            labels_list.append(0)\n","    return emails_list, labels_list\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result\n","\n","def preprocessing(text,stem=False, stop=False, sent=False):\n","    \n","    \n","    # Remove punctuations\n","    exclude = set(string.punctuation)\n","    text = ''.join(ch for ch in text if ch not in exclude)\n","    \n"," \n","    tokens = word_tokenize(text)\n","    \n","    if stop:\n","        #print(\"Stop\")\n","        stop = stopwords.words('english')\n","        stop.extend(['from', 'subject', 're', 'edu', 'use','the','but','didnt','dont','many','also','us','went','get','know','wasnt','would','one','maxaxaxaxaxaxaxaxaxaxaxaxaxaxax','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v','mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v_mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v'])\n","        #print(stop)\n","        tokens = [word.lower() for word in tokens]\n","        \n","        tokens =[word for word in tokens if len(word) > 2 or word.isnumeric() == False]        \n","            \n","        tokens =[word for word in tokens if word not in stop]\n","        \n","        #print(tokens)\n","\n","    if stem:\n","        stemmer = PorterStemmer()\n","        tokens = [stemmer.stem(t) for t in tokens]\n","    \n","    if sent:\n","        tokens = ' '.join(tokens)\n","        \n","    return tokens\n","\n","\n","\n","def clean_news(articles):\n","    \n","    clean = []\n","    dirty = []\n","    count = 0\n","    for article in articles:\n","        \n","        if len(article) > 0 and (article != '\\n'):\n","      \n","          if  article != ' ':\n","\n","            clean.append(article)\n","          else:\n","            print(\"dirty\",count)\n","            dirty.append(count)\n","        else :\n","            dirty.append(count)\n","        count += 1\n","        #print clean\n","        \n","        #sys.exit(1)\n","            \n","    return clean, dirty"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kB07YtzyQcAf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrEngUTEP7re"},"source":["datalocation = \"/content/drive/My Drive/emails/\"\n","#datalocation = \"C:/email/Documents/New folder/\"\n","emails_list, labels_list = readData(datalocation)\n","print(len(labels_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQK6T-BsOjvH"},"source":["categories1 = ['rec.sport.baseball','sci.space']\n","categories2 = ['sci.med','talk.religion.misc']\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories1, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test = fetch_20newsgroups(subset='test',categories=categories1, remove=('headers', 'footers', 'quotes'))\n","\n","newsgroups_train_cat2 = fetch_20newsgroups(subset='train', categories=categories2, remove=('headers', 'footers', 'quotes'))\n","newsgroups_test_cat2 = fetch_20newsgroups(subset='test',categories=categories2, remove=('headers', 'footers', 'quotes'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOpJXsAUOvc-"},"source":["\n","print(len(newsgroups_train.data))\n","mylist = (newsgroups_train['target_names'])\n","mylist_no = list(set(newsgroups_train['target']))\n","print(mylist)\n","print(mylist_no)\n","\n","\n","print(len(newsgroups_test.data))\n","mylist1 = (newsgroups_test['target_names'])\n","mylist_no1 = list(set(newsgroups_test['target']))\n","print(mylist1)\n","print(mylist_no1)\n","\n","print(len(newsgroups_train_cat2.data))\n","mylist_cat2 = (newsgroups_train_cat2['target_names'])\n","mylist_no_cat2 = list(set(newsgroups_train_cat2['target']))\n","print(mylist_cat2)\n","print(mylist_no_cat2)\n","\n","\n","print(len(newsgroups_test_cat2.data))\n","mylist1_cat2 = (newsgroups_test_cat2['target_names'])\n","mylist_no1_cat2 = list(set(newsgroups_test_cat2['target']))\n","print(mylist1_cat2)\n","print(mylist_no1_cat2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov-D5FVIO1NI"},"source":["targets_train = newsgroups_train.target\n","targets_test = newsgroups_test.target\n","\n","newsgroups_train1, noise_train = clean_news(newsgroups_train.data)\n","newsgroups_test1, noise_test = clean_news(newsgroups_test.data)\n","\n","targets_train_filter = np.delete(targets_train, noise_train)\n","targets_test_new_filter = np.delete(targets_test, noise_test)\n","\n","print(\"la\", len(targets_train_filter))\n","print(len(targets_test_new_filter))\n","\n","\n","targets_train_cat2 = newsgroups_train_cat2.target\n","targets_test_cat2 = newsgroups_test_cat2.target\n","\n","newsgroups_train1_cat2, noise_train_cat2 = clean_news(newsgroups_train_cat2.data)\n","newsgroups_test1_cat2, noise_test_cat2 = clean_news(newsgroups_test_cat2.data)\n","\n","targets_train_filter_cat2 = np.delete(targets_train_cat2, noise_train_cat2)\n","targets_test_new_filter_cat2 = np.delete(targets_test_cat2, noise_test_cat2)\n","\n","print(\"aa\",len(targets_train_filter_cat2))\n","print(len(targets_test_new_filter_cat2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeyA7W2MO-4t"},"source":["dataset_trainA, Labels_trainA = newsgroups_train1, targets_train_filter\n","dataset_testA, Labels_testA = newsgroups_test1, targets_test_new_filter\n","\n","\n","dataset_trainB, Labels_trainB = newsgroups_train1_cat2, targets_train_filter_cat2\n","dataset_testB, Labels_testB = newsgroups_test1_cat2, targets_test_new_filter_cat2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u116vVCqP7rj"},"source":["#input feature vector\n","vectorizer = TfidfVectorizer(tokenizer=preprocess)\n","#email_vect_C = vectorizer.fit_transform(emails_list)\n","#print(email_vect_C.shape)\n","\n","newsgroup_vect_A = vectorizer.fit_transform(dataset_trainA)\n","print(newsgroup_vect_A.shape)\n","\n","newsgroup_vect_B = vectorizer.fit_transform(dataset_trainB)\n","print(newsgroup_vect_B.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBiLdziwO94M"},"source":["import math\n","\n","def get_exp_decay(k,i,initial_lr):\n","\n","  #print(\"initial_lr: \",initial_lr)\n","  #print(\"k\", k)\n","  #print(\"itteration\", i)\n","\n","  decay = math.exp(-k*i)\n","  curr_lr = initial_lr * decay\n","\n","  #print(\"decay\", decay)\n","  #print(\"curr_lr\", curr_lr)\n","  \n","  return curr_lr, decay\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPGuadabP7rn"},"source":["#sigmoid function: Activation function used to map any real value between 0 and 1\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","#Predicts binary labels for a set of emails.\n","def predict(feature_vectors, weights):\n","    pred_sigm_list = []\n","    pred_label_list = []\n","    bias = 1\n","    for email_v in feature_vectors:\n","        weights_T = np.transpose(weights)\n","        \n","        y_pred = np.dot(email_v, weights_T) \n","        #print(y_pred.data)\n","        y_pred = y_pred.data + bias\n","        y_pred_sigm = sigmoid(y_pred)\n","        #print(\"y_pred_sigm: \",y_pred_sigm)\n","        pred_sigm_list.append(y_pred_sigm)\n","        \n","        if y_pred_sigm >= 0.5:\n","            y_pred_label = 1\n","        else:\n","            y_pred_label = 0\n","        pred_label_list.append(y_pred_label)\n","        \n","        #print(pred_sigm_list[:10])\n","        \n","    return pred_label_list, pred_sigm_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuM-e5pgP7rr"},"source":["#loss function  \n","def log_loss(y_pred_sigm, y_true):\n","    #print(y_pred_sigm)\n","    #p = np.clip(predicted, eps, 1 - eps)\n","    #print(y_pred_sigm)\n","    logloss = -y_true * np.log(y_pred_sigm) - (1 - y_true) * np.log(1 - y_pred_sigm)\n","    #print(logloss)\n","\n","    if math.isnan(logloss):\n","      logloss = np.array([0.1])\n","\n","\n","    return logloss\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_x91yqZnAsuV"},"source":["def get_fx(dw,y_pred_sigm,labels):\n","  ty = dw[0]\n","  \n","  difference = (y_pred_sigm - labels)\n","  #print(type(difference))\n","  #print(difference)\n","  if difference < 0:\n","    #print(\"(y_pred_sigm - labels) \", (y_pred_sigm - labels))\n","\n","    difference = -1 * (y_pred_sigm - labels)\n","    #print(\"difference \", difference)\n","\n","\n","  sdw = np.dot(ty, difference)\n","  return (sdw)\n","\n","def update_weights_conjugate(y_pred_sigm, labels, weights_list, emails, learning_rate):\n","\n","  g_i = get_fx(emails[0], y_pred_sigm[0], labels[0])\n","  arr = np.array([-1.0])\n","  d_i = np.dot(g_i[0], arr)\n","  flag = False\n","  \n","\n","  \n","  for i in range(len(labels)-1):\n","    weights = weights_list\n","    #print(\"Email no: \", i)\n","    #print(\"Di shape: \", d_i[0].shape)\n","    \n","    prev_fx = np.dot(d_i, y_pred_sigm[i])\n","    \n","    \n","    #print(\"prev_fx\",prev_fx)\n","    if flag:\n","      print(\"i: \", i)\n","      print(\"y_pred_sigm[i]\",y_pred_sigm[i])\n","      y_pred_sigm[i] = np.append(y_pred_sigm[i], 0.01)\n","      print(\"y_pred_sigm[i]\",y_pred_sigm[i])\n","      print(prev_fx)\n","      prev_fx = y_pred_sigm[i]\n","      emails[i+1] = emails[0] + prev_fx[0]\n","\n","\n","\n","    else:\n","      emails[i+1] = emails[i] + prev_fx[0]\n","    #print(\"Email shape: \", emails[i+1].shape)\n","    g_i_n = get_fx(emails[i+1], y_pred_sigm[i+1], labels[i+1])\n","    \n","\n","\n","    \n","    if len(g_i_n) == 0:\n","      print(\"chor\")\n","      flag = True\n","\n","      g_i_n= get_fx(emails[0], y_pred_sigm[0], labels[0])\n","      print(\"g_i_n \", g_i_n)\n","    else:\n","      flag = False\n","\n","    kaka = g_i_n[0].transpose().multiply(g_i_n[0])\n","    mama = g_i[0].transpose().multiply(g_i[0])\n","\n","    beta_i = kaka.multiply(mama)\n","\n","    kim = np.dot(g_i[0], arr)\n","    bam = beta_i.multiply(d_i[0])\n","\n","    d_i_n = kim[0] + bam [0]\n","\n","    g_i = g_i_n\n","    d_i = d_i_n\n","\n","    weights =  weights - g_i[0]\n"," \n","  return weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERR5W5gL3_m6"},"source":["def update_weights(y_pred_sigm, labels, weights_list, emails, learning_rate):\n","\n","    weights = weights_list\n","    for i in range(len(labels)):\n","        \n","        dw = np.dot(emails[i], y_pred_sigm[i] - labels[i])\n","        dw *= learning_rate\n","\n","        if len(dw) == 0:\n","\n","          #print(i)\n","          #print(y_pred_sigm[i])\n","          dw = np.dot(emails[1], 0.01 - labels[1])\n","          dw *= learning_rate\n","\n","        weights = weights - dw[0]\n","     \n","     \n","    return weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdokgtk-7022"},"source":["def gradient_descent(X, y, theta, alpha, epochs):\n","    m =len(X)\n","    J = [cost(X, y, theta)] \n","    for i in range(0, epochs):\n","        h = hypothesis(X, theta)\n","        for i in range(0, len(X.columns)):\n","            theta[i] -= (alpha/m) * np.sum((h-y)*X.iloc[:, i])\n","        J.append(cost(X, y, theta))\n","    return J, theta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMr1124QGrBU"},"source":["for i in range(len(labels)):\n","    weights = weights_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCHGtUouP7rv"},"source":["from numpy import diff\n","\n","def get_gradient(dw,y_pred_sigm,labels):\n","  ty = dw[0]\n","  \n","  difference = (y_pred_sigm - labels)\n","\n","  if difference < 0:\n","    #print(\"(y_pred_sigm - labels) \", (y_pred_sigm - labels))\n","\n","    difference = -1 * (y_pred_sigm - labels)\n","    #print(\"difference \", difference)\n","\n","\n","  sdw = np.dot(ty, difference)\n","  return (sdw)\n","\n","\n","\n","\n","def update_weights_newton(y_pred_sigm, labels, weights_list, emails, learning_rate):\n","\n","  #print(\"I am here\")\n","  for i in range(len(labels)):\n","    weights = weights_list\n","    #print(\"Weights: \", type(weights))\n","    dw = np.dot(emails[i], y_pred_sigm[i] - labels[i])\n","    \n","    if len(dw) == 0:\n","      dw = np.dot(emails[i-1], y_pred_sigm[i-1] - labels[i-1])\n","      #print(\"First Order: \", dw[0])\n","      dw2 = get_gradient(dw,y_pred_sigm[i-1],labels[i-1])\n","      #print(\"Second Order: \", dw[0])\n","    else:\n","      #print(\"First Order: \", dw[0])\n","      dw2 = get_gradient(dw,y_pred_sigm[i],labels[i])\n","      #print(\"Second Order: \", dw[0])\n","\n","    final =dw[0].multiply(dw2[0])\n","\n","    changed_weight= learning_rate *  final\n","\n","    weights = weights - changed_weight\n","    #print(\"kaka\")\n","  return weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqiudofBvEY6"},"source":["def train_lr_time_decay(emails, labels, n_iter, learning_rate,flag,k):\n","    loss_history = []\n","    \n","    lr = learning_rate\n","    decay = lr/n_iter\n","    #print(\"Decay: \", decay)\n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","    \n","    for i in range(n_iter):\n","        \n","        #Random Weights for the 1st itter\n","        if i == 0:\n","          main_weights = weights\n","        #Updated Weights for the next itters    \n","        else:\n","         # print(\"learing_rate sceduler: \", flag)\n","          if flag == \"exp_decay\":\n","            \n","            lr, decay = get_exp_decay(k,i,learning_rate)\n","          else:\n","            \n","            lr = lr / (1 + decay)\n","\n","          \n","          main_weights = updated_weight_values\n","            \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","\n","\n","      #print(\"Current Lr: \", lr)\n","       # print(pred_sigm[0])\n","        tot_loss = 0\n","        for i in range(len(labels)):\n","         \n","          y_pred_sigm = pred_sigm[i]\n","          if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","          # Compute the loss over the whole training set.\n","          indv_loss = log_loss(y_pred_sigm, labels[i])\n","          #print(\"ind_loss\",indv_loss[0])\n","          tot_loss += indv_loss[0]\n","          #print(\"tot\",tot_loss)\n","      \n","      \n","        tot_loss /= len(labels)\n","          \n","        loss_history.append(tot_loss)\n","        #updated_weight_values = update_weights(pred_sigm, labels, main_weights, emails, lr)\n","        updated_weight_values = update_weights_newton(pred_sigm, labels, main_weights, emails, lr)\n","\n","    return loss_history\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0RwPUWWP7rz"},"source":["def train(emails, labels, n_iter, learning_rate):\n","    loss_history = []\n","    \n","    \n","    #Initialize weight as random for the 1st itteration\n","    weights = np.random.randn(*emails[0].shape)\n","    weights = sparse.csr_matrix(weights[0])\n","    \n","    for i in range(n_iter):\n","        \n","        #Random Weights for the 1st itter\n","        if i == 0:\n","          main_weights = weights\n","        #Updated Weights for the next itters    \n","        else:\n","          \n","          main_weights = updated_weight_values\n","            \n","        pred_labels, pred_sigm = predict(emails, main_weights)\n","       # print(pred_sigm[0])\n","        tot_loss = 0\n","        for i in range(len(labels)):\n","         \n","          y_pred_sigm = pred_sigm[i]\n","          if len(y_pred_sigm) == 0:\n","              y_pred_sigm = np.append(y_pred_sigm, 0.01)\n","          # Compute the loss over the whole training set.\n","          indv_loss = log_loss(y_pred_sigm, labels[i])\n","          #print(\"ind_loss\",indv_loss[0])\n","          tot_loss += indv_loss[0]\n","          #print(\"tot\",tot_loss)\n","      \n","      \n","        tot_loss /= len(labels)\n","          \n","        loss_history.append(tot_loss)\n","\n","\n","        #updated_weight_values = update_weights(pred_sigm, labels, main_weights, emails, learning_rate)\n","        updated_weight_values = update_weights_newton(pred_sigm, labels, main_weights, emails, learning_rate)\n","        #updated_weight_values = update_weights_conjugate(pred_sigm, labels, main_weights, emails, learning_rate)\n","\n","    return loss_history\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCvj8HkN1W0w"},"source":["#step based decay\n","results = []\n","n_iter = 1000\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","print(lr_range)\n","\n","#corpus_sets = [email_vect_C, newsgroup_vect_A, newsgroup_vect_B]\n","corpus_title = ['email_vect_C', 'newsgroup_vect_A', 'newsgroup_vect_B']\n","#lables = [labels_list, Labels_trainA, Labels_trainB]\n","\n","for data in range(len(corpus_title)): \n","  print(\"For Dataset: \", corpus_title[data])\n","  for i in lr_range:\n","    local_results =[]\n","    print (\"For Learning rate: \", i)\n","  \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyZw5ZZsP7r3"},"source":["#step based decay\n","results = []\n","n_iter = 17\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","\n","lr_range = []\n","lr_range.append(0.01)\n","lr_range.append(0.201)\n","lr_range.append(0.501)\n","lr_range.append(0.801)\n","lr_range.append(0.999)\n","\n","#lr_scheduler = \"time_decay\"\n","lr_scheduler = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","#corpus_sets = [newsgroup_vect_B,newsgroup_vect_B,newsgroup_vect_B]\n","corpus_title = ['newsgroup_vect_B']\n","#lables = [Labels_trainB]\n","\n","for data in range(len(corpus_title)): \n","  print(\"For Dataset: \", corpus_title[data])\n","  \n","  for i in lr_range:\n","\n","    local_results =[]\n","    print (\"For Learning rate: \", i)\n","    #loss_history = train(corpus_sets[data], lables[data], n_iter, i)\n","    #loss_history_time_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i, \"time_decay\",k_hp)\n","    #loss_history_exp_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i, lr_scheduler,k_hp)\n","\n","    loss_history = np.arange(1,1000).tolist()\n","    loss_history_time_decay = np.arange(1001,2000).tolist()\n","    loss_history_exp_decay = np.arange(2001,3000).tolist()\n","\n","\n","\n","\n","\n","    local_results.append(i)\n","    local_results.append(corpus_title[data])\n","    local_results.append(loss_history)\n","    local_results.append(loss_history_time_decay)\n","    local_results.append(loss_history_exp_decay)\n","\n","\n","\n","\n","    print(\"Loss history: \", loss_history)\n","    print(\"loss_history_time_decay: \", loss_history_time_decay)\n","    print(\"loss_history_exp_decay: \", loss_history_exp_decay)\n","    results.append(local_results)\n","    pd.DataFrame(results).to_csv(\"drive/My Drive/results/Gradient_descent_default5.csv\", index = False)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ne_tLmJ6tz-b"},"source":["import pandas as pd\n","results = pd.read_csv(\"drive/My Drive/results/Gradient_descent_default_demo.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pg4mqbnalDQV"},"source":["results = results.to_numpy().tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjVKGvHupXQU"},"source":["\n","for i in range(len(results)):\n","  print(\"Result : \", results[i])\n","  print(\"Loss History Step Based: \",results[i][2] )\n","  print(\"Loss History time Based: \",results[i][3] )\n","  print(\"Loss History exp Based: \",results[i][4] )\n","\n","  print()\n","  print()\n","  \n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoD7kXJi4BH3"},"source":["  print(\"Loss History Step Based: \",len(results[0][2] ))\n","  print(\"Loss History time Based: \",len(results[0][3] ))\n","  print(\"Loss History exp Based: \",(results[0][4][0] ))\n","type(results[0][4] )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6f-snatXP4e"},"source":["results[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bWRGJy5hBVN"},"source":["results[0][2] = results[0][2].strip('[]')\n","results_str = results[0][2].split(\",\") \n","    \n","print(results_str)\n","results_flt = [float(i) for i in results_str]\n","print(results_flt)\n","print(len(results_flt))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od2J1NZzoJWe"},"source":["results[0][3] = results[0][3].strip('[]')\n","results_str_tst = results[0][3].split(\",\") \n","    \n","print(results_str_tst)\n","results_flt_tst = [float(i) for i in results_str_tst]\n","print(results_flt_tst)\n","print(len(results_flt_tst))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TLnU62yheaT"},"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","res = []\n","\n","for i in range(len(results)):\n","    results[i][2] = results[i][2].strip('[]')\n","    results_str = results[i][2].split(\",\") \n","    results_flt = [float(i) for i in results_str]\n","    res.append(results_flt)\n","\n","\n","\n","for i in range(len(lr_range)):\n","    if i % 100:\n","      plt.plot(label = str(results[i][0] ) \n","      for k in range(len(res)):\n","        plt.plot(range(len(res[k])), res[k] ) \n","      \n","      # naming the x axis , label = 'lr_range[i]'\n","        plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","        plt.ylabel('Loss') \n","      # giving a title to my graph \n","        plt.title('Loss vs Iteration Analysis') \n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTnNrMFvnBUH"},"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","\n","res = []\n","res1 = []\n","res2 = []\n","\n","for i in range(len(results)):\n","    results[i][2] = results[i][2].strip('[]')\n","    results_str = results[i][2].split(\",\") \n","    \n","\n","    results[i][3] = results[i][3].strip('[]')\n","    results_str_tst = results[i][3].split(\",\")\n","\n","    results[i][4] = results[i][4].strip('[]')\n","    results_str_exp = results[i][4].split(\",\") \n","\n","\n","    results_flt = [float(i) for i in results_str]\n","    results_flt_tst = [float(i) for i in results_str_tst]\n","    results_flt_exp = [float(i) for i in results_str_exp]\n","    res.append(results_flt)\n","    res1.append(results_flt_tst)\n","    res2.append(results_flt_exp)\n","\n","for k in range(len(res)):\n","\n","  plt.plot(range(len(res[k])), res[k] ,label = str(lr_range[k]))\n","\n","  # naming the x axis , label = 'lr_range[i]'\n","  plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","  plt.ylabel('Loss') \n","      # giving a title to my graph \n","  plt.title('Loss vs Iteration Analysis') \n","plt.legend()\n","plt.show()\n","\n","\n","#Time based Decay\n","\n","for k in range(len(res1)):\n","\n","  plt.plot(range(len(res1[k])), res1[k] ,label = str(lr_range[k]))\n","\n","  # naming the x axis , label = 'lr_range[i]'\n","  plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","  plt.ylabel('Loss') \n","      # giving a title to my graph \n","  plt.title('Loss vs Iteration Analysis') \n","plt.legend()\n","plt.show()\n","\n","\n","#Exponential Decay\n","\n","for k in range(len(res2)):\n","\n","  plt.plot(range(len(res2[k])), res2[k] ,label = str(lr_range[k]))\n","\n","  # naming the x axis , label = 'lr_range[i]'\n","  plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","  plt.ylabel('Loss') \n","      # giving a title to my graph \n","  plt.title('Loss vs Iteration Analysis') \n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6HoHcVfP4AX"},"source":["#Time based Decay\n","\n","for k in range(len(res1)):\n","\n","  plt.plot(range(len(res1[k])), res1[k] ,label = str(lr_range[k]))\n","\n","  # naming the x axis , label = 'lr_range[i]'\n","  plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","  plt.ylabel('Loss') \n","      # giving a title to my graph \n","  plt.title('Loss vs Iteration Analysis') \n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILai8WZ3Sz3o"},"source":["for i in range(len(res1)):\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5a4Hde61P7r-"},"source":["from matplotlib import pyplot\n","for i in range(len(lr_range)):\n","    if i % 100:\n","       pyplot.plot(range(n_iter), results[i][4], label = str(lr_range[i])) \n","      \n","      # naming the x axis , label = 'lr_range[i]'\n","       plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","       plt.ylabel('Loss') \n","      # giving a title to my graph \n","       plt.title('Loss vs Iteration Analysis') \n","pyplot.legend()\n","pyplot.show()\n","\n","for i in range(len(lr_range)):\n","    if i % 100:\n","       pyplot.plot(range(n_iter), results[i][3], label = str(lr_range[i])) \n","      \n","      # naming the x axis , label = 'lr_range[i]'\n","       plt.xlabel('Number of Iterations') \n","      # naming the y axis \n","       plt.ylabel('Loss') \n","      # giving a title to my graph \n","       plt.title('Loss vs Iteration Analysis') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VsKNwOqP7sC"},"source":["results = []\n","n_iter = 1000\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","\n","lr_range = list(np.arange(min_lr, max_lr, lr_step))\n","print(lr_range)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4XRkQuywmu57"},"source":["import matplotlib\n","print (matplotlib.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZFCNWXwnN7e"},"source":["#step based decay\n","results = []\n","n_iter = 17\n","min_lr = 0.001\n","max_lr =1.0\n","lr_step = 0.1\n","\n","lr_range = []\n","lr_range.append(.01)\n","lr_range.append(.101)\n","\n","#lr_scheduler = \"time_decay\"\n","lr_scheduler = \"exp_decay\"\n","#hyperparameter for exp_decay\n","#also have to tune to range\n","k_hp = 0.1\n","\n","#corpus_sets = [newsgroup_vect_B,newsgroup_vect_B,newsgroup_vect_B]\n","corpus_title = ['email_vect_C','newsgroup_vect_A','newsgroup_vect_B']\n","#lables = [Labels_trainB]\n","\n","for data in range(len(corpus_title)): \n","  print(\"For Dataset: \", corpus_title[data])\n","  \n","  for i in lr_range:\n","\n","    local_results =[]\n","    print (\"For Learning rate: \", i)\n","    #loss_history = train(corpus_sets[data], lables[data], n_iter, i)\n","    #loss_history_time_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i, \"time_decay\",k_hp)\n","    #loss_history_exp_decay = train_lr_time_decay(corpus_sets[data], lables[data], n_iter, i, lr_scheduler,k_hp)\n","\n","    loss_history = np.arange(1,17).tolist()\n","    loss_history_time_decay = np.arange(20,37).tolist()\n","    loss_history_exp_decay = np.arange(40,57).tolist()\n","\n","    local_results.append(i)\n","    local_results.append(corpus_title[data])\n","    local_results.append(loss_history)\n","    local_results.append(loss_history_time_decay)\n","    local_results.append(loss_history_exp_decay)\n","\n","    print(\"Loss history: \", loss_history)\n","    print(\"loss_history_time_decay: \", loss_history_time_decay)\n","    print(\"loss_history_exp_decay: \", loss_history_exp_decay)\n","    results.append(local_results)\n","    pd.DataFrame(results).to_csv(\"drive/My Drive/Results/Gradient_descent_Logit_Defaultparameter.csv\", index = False)\n","    \n","    "],"execution_count":null,"outputs":[]}]}